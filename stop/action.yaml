name: 'Falco Action Stop'
description: 'Run Falco in a GitHub Action'
author: 'The Falco Authors'
inputs:
  custom_rule_file:
    description: 'Custom rule file'
    required: false
    default: ''
  profile_enabled: 
    description: 'Enable profiling. Requires write access to repository'
    type: boolean 
    default: false
  profile_dir:
    description: 'Profile dir in repository (Default .falco)'
    default: '.falco'
  save_capture:
    description: "Save Files on S3?"
    type: boolean
    default: false
  openAI_model:
    description: "Open AI model to use for summary"
    type: sting
    default: "gpt-3.5-turbo"
  openAI_user_prompt:
    description: "Message to send to OpenAI"
    type: string
  verbose:
    description: "Enable verbose logs"
    type: boolean
    default: false
runs:
  using: 'composite'
  steps:
  - name: Stop Sysdig capture
    shell: bash
    run: |
      echo "Stopping Sysdig"
      docker stop sysdig
      echo "Sysdig stopped"
  
  - name: Start Falco in capture mode
    shell: bash
    run: |
      MOUNT_CUSTOM_RULE=""
      if [[ -f "${{inputs.custom_rule_file}}" ]]; then
        if [ "${{ inputs.verbose }}" = "true" ]; then
          echo "Loading custom rules from ${{inputs.custom_rule_file}}"
        fi
        MOUNT_CUSTOM_RULE="-v ${{inputs.custom_rule_file}}:/etc/falco/rules.d/custom_rules.yaml"
      fi

      docker run --rm --name falco --privileged \
        -v /var/run/docker.sock:/host/var/run/docker.sock \
        -v /proc:/host/proc:ro -v /etc:/host/etc:ro \
        -v /tmp:/tmp \
        $MOUNT_CUSTOM_RULE \
        falcosecurity/falco-no-driver:latest falco -o "json_output=true" -o "file_output.enabled=true" -o "file_output.keep_alive=false" -o "file_output.filename=/tmp/falco_events.json" -o "engine.kind=replay" -o "engine.replay.capture_file=/tmp/capture.scap" 

  - name: Extract outbound connections
    shell: bash
    env:
      VERBOSE: ${{ inputs.verbose }}
    run: |
      OUTBOUND_FILTER="(((evt.type = connect and evt.dir=<) or (evt.type in (sendto,sendmsg,sendmmsg) and evt.dir=< and fd.l4proto != tcp and fd.connected=false and fd.name_changed=true)) and (fd.typechar = 4 or fd.typechar = 6) and (fd.ip != \"0.0.0.0\" and fd.net != \"127.0.0.0/8\" and not fd.snet in (\"10.0.0.0/8\", \"172.16.0.0/12\", \"192.168.0.0/16\"))) and not proc.name in (pythonist, dragent, ssm-agent-worke) and not proc.pname in (eic_curl_author, dhclient-script)" 
      OUTBOUND_OUTPUT="%fd.sip,%fd.sport,%proc.name,%proc.exepath,%user.name"
      ${{github.action_path}}/src/run_sysdig.sh filter "/tmp/capture.scap" "$OUTBOUND_FILTER" "$OUTBOUND_OUTPUT" "/tmp/outbound.txt"

  - name: Extract written files
    shell: bash
    env:
      VERBOSE: ${{ inputs.verbose }}
    run: |
      FILE_WRITTEN_FILTER="evt.type in (open,openat,openat2) and evt.is_open_write=true and fd.typechar=\"f\" and fd.num>=0 and ( not fd.name startswith \"/var/lib/docker/\" and not fd.name startswith \"/home/runner/work/_temp/_runner_file_commands/\") and not (fd.name startswith \"/home/runner/runners/\" and proc.exepath endswith \"/bin/Runner.Worker\" and proc.pexepath endswith \"/bin/Runner.Listener\")"
      FILE_WRITTEN_OUTPUT="%fd.name,%proc.name,%proc.exepath,%proc.pexepath,%user.name"
      ${{github.action_path}}/src/run_sysdig.sh filter "/tmp/capture.scap" "$FILE_WRITTEN_FILTER" "$FILE_WRITTEN_OUTPUT" "/tmp/files_written.txt"

  - name: Extract processes information
    shell: bash
    env:
      VERBOSE: ${{ inputs.verbose }}
    run: |
      PROC_FILTER="evt.type in (execve, execveat) and evt.dir=< and evt.arg.res=0"
      PROC_FILTER_OUTPUT="%proc.name,%proc.exepath,%proc.pname,%proc.pexepath,%user.name"
      ${{github.action_path}}/src/run_sysdig.sh filter "/tmp/capture.scap" "$PROC_FILTER" "$PROC_FILTER_OUTPUT" "/tmp/proc.txt"

  - name: Extract chisels information for summary
    shell: bash
    env:
      VERBOSE: ${{ inputs.verbose }}
    run: |
      ${{github.action_path}}/src/run_sysdig.sh chisel "/tmp/capture.scap" "udp_extract.lua" "/tmp/udp_extract.txt"
      ${{github.action_path}}/src/run_sysdig.sh chisel "/tmp/capture.scap" "topconns.lua" "/tmp/top_connection.txt"
      ${{github.action_path}}/src/run_sysdig.sh chisel "/tmp/capture.scap" "topprocs_net.lua" "/tmp/topprocs_net.txt"

  - name: Extract DNS domains
    shell: bash
    env:
      VERBOSE: ${{ inputs.verbose }}
    run: |
      DNS_FILTER="evt.type in (recvmsg,read,recv,recvfrom) and fd.rport=53 and fd.l4proto=udp"
      DNS_OUTPUT="%evt.buffer"
      ${{github.action_path}}/src/run_sysdig.sh filter "/tmp/capture.scap" "$DNS_FILTER" "$DNS_OUTPUT" "/tmp/buffers.txt"
      if [[ ! -s /tmp/buffers.txt ]]; then
        echo "Warning: No contacted DNS domains found"
        exit 0
      fi
      cat /tmp/buffers.txt | grep -o -E "[a-zA-Z0-9-]+(\.[a-zA-Z0-9-]+)*(\.[a-zA-Z]{2,})" | grep -E -v "NULL" | grep -E -v "ns." | grep -E -v "\->" | grep -E -v "evt.buffer" | sort | uniq > /tmp/dns_extract.txt
      if [[ "${{ inputs.verbose }}" == "true" ]]; then
        echo "Extracted DNS domains" && cat /tmp/dns_extract.txt
      fi
      while read -r line; do
        echo "{ \"domain\": \"$line\" }"
      done < /tmp/dns_extract.txt > /tmp/dns_extract_json.txt
  
  - name: Extract executables' hash
    shell: bash
    env:
      VERBOSE: ${{ inputs.verbose }}
    run: |      
      cat /tmp/proc.txt| jq '."proc.exepath"' | tr -d "\"" | while read -r file; do
        if [ -f "$file" ]; then
            line=$(sha256sum "$file")
            sha=$(echo "$line" | awk '{print $1}')
            file=$(echo "$line" | awk '{print $2}')
            echo "{ \"sha256\": \"$sha\", \"filename\": \"$file\" }"
        else
            echo "Warning: File $file does not exist anymore when extracting its SHA256 hash" >&2
        fi 
      done > /tmp/hashes.txt

      sort -u -o /tmp/hashes.txt /tmp/hashes.txt
  
  - name: Extract docker container images
    shell: bash
    env:
      VERBOSE: ${{ inputs.verbose }}
    run: |
      CONTAINER_FILTER="evt.type=container"
      CONTAINER_OUTPUT="%container.name,%container.image.repository"
      ${{github.action_path}}/src/run_sysdig.sh filter "/tmp/capture.scap" "$CONTAINER_FILTER" "$CONTAINER_OUTPUT" "/tmp/containers.txt"
      if [[ -s /tmp/containers.txt ]]; then
        sort -u -o /tmp/containers.txt /tmp/containers.txt
        echo "Sorted /tmp/containers.txt"
      else
        echo "File /tmp/containers.txt is empty. Impossible to sort it."
      fi
      

  - name: Retrieve Job Info using GitHub API
    shell: bash
    env:
      RUN_ID: ${{ github.run_id }}
      REPO: ${{ github.repository }}
    run: |
      echo "Retrieving details for run_id=$RUN_ID"
      curl -s -H "Authorization: Bearer $GITHUB_TOKEN" \
        -H "Accept: application/vnd.github.v3+json" \
        https://api.github.com/repos/$REPO/actions/runs/$RUN_ID/jobs > jobs.json
      
      # Parse JSON and retrieve step info with timestamps
      echo "Job details:"
      jq -c '.jobs[] | {job_id: .id, job_name: .name, steps: .steps[] | {name: .name, started_at: .started_at, completed_at: .completed_at, status: .status}}' jobs.json > steps.json

      if [[ "${{ inputs.verbose }}" == "true" ]]; then
        cat steps.json
      fi
      mv steps.json /tmp/steps.json
      
  - name: Build summary
    shell: bash
    run: |
      # Falco events
      if [[ -s /tmp/falco_events.json ]]; then
        if ${{ github.event_name == 'pull_request' }}; then
          echo "Creating PR comment with triggered signatures"
          ${{github.action_path}}/pr-comment.sh /tmp/falco_events.json ${{ github.token }}
        else
          echo "Printing triggered rules in Job summary"
          echo "# Report Details" >> /tmp/report_output.md
          echo "## Falco Events" >> /tmp/report_output.md
          python3 ${{github.action_path}}/src/falco_events_to_md.py /tmp/falco_events.json /tmp/steps.json >> /tmp/report_output.md
        fi
      fi

      # Processes
      echo "## Processes" >> /tmp/report_output.md
      python3 ${{github.action_path}}/src/json_to_md.py /tmp/proc.txt >> /tmp/report_output.md

      # Written files
      echo "## Written Files" >> /tmp/report_output.md
      python3 ${{github.action_path}}/src/json_to_md.py /tmp/files_written.txt >> /tmp/report_output.md

      # Contacted IPs
      echo "## Contacted IPs" >> /tmp/report_output.md
      python3 ${{github.action_path}}/src/json_to_md.py /tmp/outbound.txt >> /tmp/report_output.md

      # DNS
      echo "# Contacted DNS Domains" >> /tmp/report_output.md
      python3 ${{github.action_path}}/src/json_to_md.py /tmp/dns_extract_json.txt >> /tmp/report_output.md

      # Containers
      echo "# Containers" >> /tmp/report_output.md
      python3 ${{github.action_path}}/src/json_to_md.py /tmp/containers.txt >> /tmp/report_output.md

      # Hashes
      echo "# Executable Hashes" >> /tmp/report_output.md
      python3 ${{github.action_path}}/src/json_to_md.py /tmp/hashes.txt >> /tmp/report_output.md

      # Top Connections
      echo "## Top Connections" >> /tmp/report_output.md
      python3 ${{github.action_path}}/src/capture_to_md.py /tmp/top_connection.txt >> /tmp/report_output.md
      echo "" >> /tmp/report_output.md

      if ${{ env.OPENAI_API_KEY != '' }}; then
        echo "Creating Report Summary using OpenAI"
        echo "# Report Summary" >> $GITHUB_STEP_SUMMARY
        echo "Installing python dependencies"
        python3 -m pip install -r ${{github.action_path}}/src/integrations/openai/requirements.txt
        echo "Calling OpenAI APIs for summary"
        if ${{inputs.openAI_user_prompt == ''}}; then
          python3 ${{github.action_path}}/src/integrations/openai/create_summary.py /tmp/report_output.md --model "${{inputs.openAI_model}}" >> $GITHUB_STEP_SUMMARY
        else
          python3 ${{github.action_path}}/src/integrations/openai/create_summary.py /tmp/report_output.md --model "${{inputs.openAI_model}}" --user_input "${{inputs.openAI_user_prompt}}" >> $GITHUB_STEP_SUMMARY
        fi
      fi

      echo "Appending report to github summary"
      cat /tmp/report_output.md >> $GITHUB_STEP_SUMMARY

  - name: Push profile to repository
    if: ${{ inputs.profile_enabled == 'true'}}
    shell: bash
    run: |
      PROFILE_PATH="$GITHUB_WORKSPACE/${{inputs.profile_dir}}"
      echo "$PROFILE_PATH"
      if [ ! -d "PROFILE_PATH" ]; then
        echo "Creating profile dir"
        mkdir -p "./${{inputs.profile_dir}}"
      fi
      
      ls -la ${{inputs.profile_dir}}/
      cp /tmp/proc.txt ${{inputs.profile_dir}}/profile_proc.txt
      cp /tmp/files_written.txt ${{inputs.profile_dir}}/profile_files_written.txt
      cp /tmp/outbound.txt ${{inputs.profile_dir}}/profile_conn.txt
      cp /tmp/dns_extract_json.txt ${{inputs.profile_dir}}/profile_dns.txt
      cp /tmp/hashes.txt ${{inputs.profile_dir}}/profile_hashes.txt
      cp /tmp/containers.txt ${{inputs.profile_dir}}/profile_containers.txt
      
      ${{github.action_path}}/src/push_profile.sh

  - name: Save captures and events on S3 bucket
    if: ${{ inputs.save_capture == 'true' && env.AWS_ACCESS_KEY_ID != '' && env.AWS_SECRET_ACCESS_KEY != '' && env.S3_BUCKET != ''}}
    shell: bash
    run: |
      sudo apt-get update
      sudo apt-get install -y awscli
 
      aws configure set aws_access_key_id ${{ env.AWS_ACCESS_KEY_ID }}
      aws configure set aws_secret_access_key ${{ env.AWS_SECRET_ACCESS_KEY }}
      aws configure set default.region ${{ env.AWS_DEFAULT_REGION }}

      FOLDER_NAME=$(date +"%Y%m%d_%H%M%S")
      CAPTURE_FILE_NAME="capture_$FOLDER_NAME.scap"
      EVENT_FILE_NAME="events_$FOLDER_NAME.json"
      S3_DESTINATION_FOLDER="s3://${{ env.AWS_S3_BUCKET_NAME }}/$FOLDER_NAME"
      aws s3 cp /tmp/capture.scap "$S3_DESTINATION_FOLDER/$CAPTURE_FILE_NAME"